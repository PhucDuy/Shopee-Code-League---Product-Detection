{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.style as style\nfrom PIL import Image\nimport seaborn as sns\n\nnp.random.seed(115)\nfrom multiprocessing import cpu_count\nnCores = cpu_count()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U efficientnet\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport cv2\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nprint(tf.__version__)\n\n# import our model, different layers and activation function \nfrom tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport logging\nimport warnings\nimport gc\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Activation,GlobalAveragePooling2D,BatchNormalization,Dropout,MaxPooling2D\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout,GlobalAveragePooling2D\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.applications.xception import preprocess_input\n\ntf.random.set_seed(115)\nwarnings.filterwarnings('ignore')\nlogging.getLogger('tensorflow').setLevel(logging.INFO)\nimport numpy as np\n# from tensorflow.keras.mixed_precision import experimental as mixed_precision\nimport tensorflow_addons as tfa\nimport efficientnet.tfkeras as efn\n\n# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n# policy = mixed_precision.Policy('float32')\n# mixed_precision.set_policy(policy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = '../input/shopee-product-detection-open'\ntrain_folder_path = os.path.join(root_path,'train/train/train')\nprint(train_folder_path)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(os.path.join(root_path,'train.csv'))\nprint(train.info())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"working_path = os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfull_train = True\nif not full_train: \n    _ ,dataset = train_test_split(train,test_size=0.05,random_state=45,stratify=train['category'])\nelse:    \n    dataset = train\n#delete when no longer needed\ndel train\n#collect residual garbage\ngc.collect()\n\nprint(dataset.info())\nprint(dataset.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CountStatus = dataset['category'].value_counts()\n# CountStatus.plot(figsize=(10,10));\n# CountStatus.plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                  np.unique(dataset['category']),\n                                                  dataset['category'])\n\n# Convert class_weights to a dictionary to pass it to class_weight in model.fit\nclass_weights = dict(enumerate(class_weights))\nprint(class_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['filename'] = dataset.apply(lambda x : os.path.join(os.path.join(train_folder_path,str(x.category).zfill(2)),x.filename) ,axis=1)\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# image_path = dataset.sample(1)['filename']\n# print(image_path.iloc[0])\n# img = cv2.imread(image_path.iloc[0])\n# plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df ,test_df = train_test_split(dataset,test_size=0.1,random_state=45,stratify=dataset['category'])\n#delete when no longer needed\ndel dataset\n#collect residual garbage\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataset_from_dataframe(df):\n    ds = tf.data.Dataset.from_tensor_slices((df['filename'],df['category']))\n    ds = ds.shuffle(buffer_size=len(df))\n    return ds\ntrain_ds = dataset_from_dataframe(train_df)\nval_ds = dataset_from_dataframe(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CHANNELS = 3\n# BATCH_SIZE = 48\n\n# Configuration\nBATCH_SIZE = 64\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nMAX_EPOCHS = 10\nLR = 1e-5\nimg_size= 300\nbuffer_size = 2048\nNUM_CLASSES = 42\nSTEPS_PER_TRAIN_EPOCH = tf.math.ceil(train_df.shape[0]/BATCH_SIZE)\nSTEPS_PER_TEST_EPOCH = tf.math.ceil(test_df.shape[0]/BATCH_SIZE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef preprocess(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [img_size,img_size])\n    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n    image = tf.image.convert_image_dtype(image, tf.float32)    \n    image = (image*2) - 1  # normalize to [-1,1] range\n    image = tf.image.per_image_standardization(image)\n    return image\n\n\ndef random_brightness(image):\n    return tf.image.random_brightness(image, .4)\n\ndef random_contrast(image):\n    return tf.image.random_contrast(image, .3,1.7)\n\ndef random_flip_left_right(image):\n    return tf.image.random_flip_left_right(image)\n\ndef random_crop(image):\n    image = tf.image.resize_with_crop_or_pad(image, img_size+28, img_size+28) # Add 6 pixels of padding\n    image = tf.image.random_crop(image,[img_size,img_size,3])\n    return image\n\ndef random_hue(image):\n    return tf.image.random_hue(image, 0.05)\ndef random_flip_up_down(image):\n    return tf.image.random_flip_up_down(image)\ndef random_sataration(image):\n    return tf.image.random_saturation(image, 0.6, 1.6)\n\n\naugs_color = [random_brightness, random_contrast, random_hue,random_sataration]\naugs_str = [random_flip_left_right, random_flip_up_down, random_crop]\n\ndef augmentation(image, label):\n    augs = augs_color + augs_str\n    random.shuffle(augs)\n    k = random.randint(1,len(augs))\n    augs = random.choices(augs, k=4)\n    for i in augs:\n        image = i(image) \n    return image, label\n\ndef load_and_preprocess_from_path_and_label(path,label):\n    return preprocess(path), label\n\ndef prepare_for_training(ds, cache=True,shuffle_buffer_size=100,augment=False):\n    if cache:\n        if isinstance(cache,str):\n            ds = ds.cache(cache)\n        else: \n            ds = ds.cache()\n    if shuffle_buffer_size > 0:\n        ds = ds.shuffle(buffer_size=shuffle_buffer_size)    \n    # repeat forever\n    ds = ds.repeat()\n    if augment:\n        ds.map(augmentation, num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE)\n    \n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = train_ds.map(lambda path,label: load_and_preprocess_from_path_and_label(path,label),\n                       num_parallel_calls=AUTOTUNE)\nval_ds = val_ds.map(lambda path,label: load_and_preprocess_from_path_and_label(path,label),                      \n                       num_parallel_calls=AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = prepare_for_training(train_ds,shuffle_buffer_size=buffer_size,augment=True,cache=False)\nval_ds = prepare_for_training(val_ds,shuffle_buffer_size=buffer_size,cache=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(load_from_path=None,hparams=None):\n    if load_from_path == None:\n        \n        base_model =  efn.EfficientNetB3(weights='noisy-student', \n                                         include_top=False, \n                                         pooling='max', \n                                         classes=NUM_CLASSES,\n                                         input_shape=(img_size,img_size, 3))\n        base_model.trainable= True\n        \n        print(\"Number of layers in the base model:\", len(base_model.layers))\n        fine_tune_at = int(0.9 * len(base_model.layers))\n\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n\n        model = tf.keras.Sequential([\n          base_model,\n          BatchNormalization(),\n          Dropout(0.25),\n          Dense(units=512, activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(1e-2)),\n          Dropout(0.5),\n          Dense(units=512, activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(1e-2),),\n          Dropout(0.5),\n#           Dense(units=256, activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(1e-3),),\n#           Dropout(0.2),\n          Dense(NUM_CLASSES,activation='softmax')\n        ])\n        model.compile(optimizer='nadam',\n                      loss='sparse_categorical_crossentropy',\n                      metrics=[\"accuracy\"])\n    else:\n        model = tf.keras.models.load_model(load_from_path)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model('../input/model-efnb3-noisy-3drop-2selu-ndam/model_efnb3_noisy_3drop_2selu_ndam.h5')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('training/cp.ckpt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                             mode='min', patience=10,\n                                             verbose=1) # Create EarlyStopping Callback\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                 factor=0.25,\n                                                 patience=1, \n                                                 min_lr=1e-13)\n\n\ncheckpoint_path = \"training/cp.ckpt\"\n\n# Create a callback that saves the model's weights\n# by default it saves the weights every epoch\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_best_only=True,\n                                                 save_weights_only=True,\n                                                 verbose=1)\n\nhistory = model.fit(train_ds, epochs=MAX_EPOCHS,     \n                    validation_steps=STEPS_PER_TEST_EPOCH,\n                    steps_per_epoch=STEPS_PER_TRAIN_EPOCH,\n                    validation_data=val_ds,\n                    callbacks=[cp_callback,reduce_lr,earlystop], # Add callback to training process\n                    class_weight=class_weights,\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(os.path.join(working_path,'model_efnb3_noisy_3drop_2selu_ndam.h5'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_folder_path = os.path.join(root_path,'test/test/test')\nprint(test_folder_path)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(os.path.join(root_path,'test.csv'))\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['filename'] = dd.from_pandas(test,npartitions=nCores).\\\n   map_partitions(\n      lambda df : df.apply(\n         lambda x : os.path.join(test_folder_path,x.filename) ,axis=1)).\\\n   compute(scheduler='processes')\ntest.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = dataset_from_dataframe(test)\ntest_ds = test_ds.map(lambda path,label: preprocess(path),                      \n                       num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = model.predict(test_ds, batch_size=BATCH_SIZE,verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape[0])\nprint(res.shape[0])\nmax_res = np.argmax(res, axis=1)\nprint(max_res.shape[0])\nprint(max_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.read_csv(os.path.join(root_path,'test.csv'))\nprint(output.shape[0])\noutput['category'] = max_res\noutput['category'] = output.category.apply(lambda c: str(c).zfill(2))\npath = os.path.join(working_path,'submission.csv')\nprint(output.info())\nprint(path)\noutput.to_csv(path, index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}